{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from bpemb import BPEmb\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from model.create_mask import *\n",
    "from training.printing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Constants"
   ]
  },
  {
   "source": [
    "## 1.1. Paths"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/run/media/ishrak/Ishrak/IUT/Thesis/dataset/tfrecords/\"\n",
    "TRAIN_DATASET_PATH = os.path.join(DATASET_PATH, \"train\")\n",
    "TEST_DATASET_PATH = os.path.join(DATASET_PATH, \"test\")\n",
    "MODEL_DIR = \"/run/media/ishrak/Ishrak/IUT/Thesis/model_dir\"\n",
    "CKPT_PATH = os.path.join(MODEL_DIR, \"ckpt\")\n",
    "LOG_PATH = os.path.join(MODEL_DIR, \"logs\")\n",
    "SCORE_PATH = os.path.join(LOG_PATH, \"score.csv\")\n",
    "LOG_TEXT_PATH = os.path.join(LOG_PATH, \"log.txt\")\n",
    "TENSORBOARD_PATH = os.path.join(LOG_PATH, \"tensorboard_logs\")"
   ]
  },
  {
   "source": [
    "## 1.2. Constant values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2 ** 13\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "NUM_LAYERS = 4\n",
    "D_MODEL = 128\n",
    "DFF = 512\n",
    "NUM_HEADS = 8\n",
    "SUMMARY_LENGTH = 16\n",
    "TEXT_LENGTH = 512\n",
    "START_TOKEN = 1\n",
    "END_TOKEN = 2\n",
    "VOCAB_SIZE = 10000\n",
    "ENCODER_VOCAB_SIZE = VOCAB_SIZE\n",
    "DECODER_VOCAB_SIZE = VOCAB_SIZE\n",
    "VOCAB_DIM = 100\n",
    "CKPT_TO_KEEP = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yH5cg5pSIHaZ"
   },
   "source": [
    "# 2. Loading Dataset\n",
    "The dataset consists of articles scraped from Prothom Alo news site. The dataset contains titles, contents and tags of many article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecord_files = [\n",
    "    os.path.join(TRAIN_DATASET_PATH, file_name)\n",
    "    for file_name in os.listdir(TRAIN_DATASET_PATH)\n",
    "]\n",
    "test_tfrecord_files = [\n",
    "    os.path.join(TEST_DATASET_PATH, file_name)\n",
    "    for file_name in os.listdir(TEST_DATASET_PATH)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_manipulation.create_tfrecord_dataset import create_tfrecord_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_tfrecord_dataset(\n",
    "    tfrecord_files=train_tfrecord_files,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    cache_buffer_size=BUFFER_SIZE,\n",
    "    prefetch_buffer_size=tf.data.experimental.AUTOTUNE,\n",
    "    input_feature_length=TEXT_LENGTH,\n",
    "    output_feature_length=SUMMARY_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = create_tfrecord_dataset(\n",
    "    tfrecord_files=test_tfrecord_files,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    cache_buffer_size=BUFFER_SIZE,\n",
    "    prefetch_buffer_size=tf.data.experimental.AUTOTUNE,\n",
    "    input_feature_length=TEXT_LENGTH,\n",
    "    output_feature_length=SUMMARY_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi-jAU3zJE9N"
   },
   "source": [
    "# 2. Model"
   ]
  },
  {
   "source": [
    "## 2.1. Model Architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXHRG-o4R9Mc"
   },
   "outputs": [],
   "source": [
    "from model.transformer import Transformer\n",
    "transformer = Transformer(\n",
    "    NUM_LAYERS, \n",
    "    D_MODEL, \n",
    "    NUM_HEADS, \n",
    "    DFF,\n",
    "    ENCODER_VOCAB_SIZE, \n",
    "    DECODER_VOCAB_SIZE, \n",
    "    pe_input=ENCODER_VOCAB_SIZE, \n",
    "    pe_target=DECODER_VOCAB_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOGvkYDNTjIj"
   },
   "source": [
    "## 2.2. Adam optimizer\n",
    "Used adam optimizer with custom learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.custom_scheduler import CustomSchedule\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "source": [
    "## 2.3. Checkpoints"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, CKPT_PATH, max_to_keep=CKPT_TO_KEEP)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UndsMPZXTdSr"
   },
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsVdrENTUERY"
   },
   "source": [
    "## 3.1. Defining losses and other metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktKwyvKtTvF6"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uW4LA_45T4Aa"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze0u6xxXT7dI"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfpI0gS4c06c"
   },
   "source": [
    "### Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmVOMzkrczgl"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, \n",
    "            tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(model, input, target):\n",
    "    tar_inp = target[:, :-1]\n",
    "    tar_real = target[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    predictions, _ = transformer(\n",
    "        input, \n",
    "        tar_inp, \n",
    "        False, \n",
    "        enc_padding_mask, \n",
    "        combined_mask, \n",
    "        dec_padding_mask\n",
    "    )\n",
    "    loss = loss_object(tar_real, predictions)\n",
    "\n",
    "    test_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMbqGTixu1cl"
   },
   "source": [
    "## 3.2. Inference function\n",
    "Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5D5cv2Jd8-6"
   },
   "outputs": [],
   "source": [
    "def evaluate(input_document):\n",
    "    input_document = [bpemb_bn.encode_ids(input_document)]\n",
    "    input_document = pad_sequences(input_document, maxlen=TEXT_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
    "\n",
    "    decoder_input = [START_TOKEN]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(SUMMARY_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == END_TOKEN:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkpdiW6wnmiS"
   },
   "outputs": [],
   "source": [
    "def summarize(input_document):\n",
    "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
    "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
    "    return bpemb_bn.decode_ids(summarized[0])  # since there is just one translated document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLV7KftyqvQ0"
   },
   "source": [
    "### Test inference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_bn = BPEmb(lang = \"bn\", vs = VOCAB_SIZE, dim = VOCAB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCBOQvzcpLxR"
   },
   "outputs": [],
   "source": [
    "TEST_SUMMARY = \"ট্রাকের ধাক্কায় সড়কে ছিটকে পড়ে প্রবাসীর মৃত্যু\"\n",
    "TEST_CONTENT = \"যশোরের বাঘারপাড়া উপজেলায় ট্রাকের ধাক্কায় এক প্রবাসীর মৃত্যু হয়েছে। তাঁর নাম আবু সাঈদ (৪০)। রোববার রাত আটটার দিকে উপজেলার খাজুরায় বাঘারপাড়া-কালীগঞ্জ সড়কে আরিফ ব্রিকসের সামনে এ দুর্ঘটনা ঘটে।\"\\\n",
    "\" নিহত আবু সাঈদ বাঘারপাড়া উপজেলার বন্দবিলা ইউনিয়নের দাঁতপুর গ্রামের সদর আলী দফাদারের ছেলে। তিনি মালয়েশিয়াপ্রবাসী ছিলেন। প্রত্যক্ষদর্শীর বরাত দিয়ে পুলিশ জানায়, রোববার রাতে খাজুরা বাজার থেকে বাঘারপাড়া-কালীগঞ্জ সড়ক দিয়ে\"\\\n",
    "\" বাইসাইকেলে করে বাড়ি ফিরছিলেন আবু সাঈদ। রাত আটটার দিকে তিনি আরিফ ব্রিকসের সামনে পৌঁছান। এ সময় পেছন দিক থেকে আসা বাঘারপাড়াগামী একটি দ্রুতগামী ট্রাক সাইকেলটিকে ধাক্কা দেয়। বাইসাইকেল থেকে সড়কের ওপর ছিটকে পড়ে\"\\\n",
    "\" সেখানেই সাঈদের মৃত্যু হয়। আবু সাঈদের ভাই রেজাউল দফাদার বলেন, আবু সাঈদ মালয়েশিয়ায় চাকরি করতেন। সম্প্রতি ছুটিতে তিনি দেশে এসেছিলেন।\"\\\n",
    "\" বাঘারপাড়া থানা ভারপ্রাপ্ত কর্মকর্তা (ওসি) সৈয়দ আল মামুন বলেন, ট্রাকের ধাক্কায় আবু সাঈদ নামের এক বাইসাইকেলচালক নিহত হয়েছেন।\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "KcyUe-mK2Ytr",
    "outputId": "0780c114-fd10-4a4d-d868-f42e5be8ae3e"
   },
   "outputs": [],
   "source": [
    "summary = summarize(TEST_CONTENT)\n",
    "summary"
   ]
  },
  {
   "source": [
    "## 3.3. Tensorboard Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = os.path.join(TENSORBOARD_PATH,  current_time + '/train')\n",
    "test_log_dir = os.path.join(TENSORBOARD_PATH, current_time + '/test')\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTiHAI29Sat6"
   },
   "source": [
    "## 3.4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print_header(LOG_TEXT_PATH)\n",
    "step = 1\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    \n",
    "    # training loop\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss = train_loss.result()\n",
    "            summary = summarize(TEST_CONTENT)\n",
    "            print_info(epoch, batch, loss, summary, LOG_TEXT_PATH, start)\n",
    "            save_score(SCORE_PATH, epoch, batch, loss)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar(\"per_100_batch_loss\", loss, step = step)\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", loss, step = epoch)\n",
    "\n",
    "    # testing loop\n",
    "    for (inp, tar) in test_dataset:\n",
    "        test_step(inp, tar)\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", test_loss.result(), step = epoch)\n",
    "        step += 1\n",
    "        \n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "ku8AxkGg0ZE0",
    "XWsD8vO20bwf",
    "VMH_meJWn0-5",
    "9vVLiJrirnpz"
   ],
   "name": "Thesis_Model_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}